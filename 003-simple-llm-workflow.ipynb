{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Building a Question Answering Pipeline Using LangGraph and OpenAI LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required components from LangGraph and LangChain-OpenAI\n",
    "from langgraph.graph import StateGraph, START, END  # Workflow graph and entry/exit points\n",
    "from langchain_openai import ChatOpenAI             # LLM wrapper for OpenAI models\n",
    "from typing import TypedDict                        # For strong type annotation of state objects\n",
    "from dotenv import load_dotenv                      # To read OpenAI (or other) keys from a .env file\n",
    "\n",
    "# Load environment variables (OPENAI_API_KEY) from a .env file.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the OpenAI language model. Here, we specify the model as \"gpt-5\".\n",
    "model = ChatOpenAI(model=\"gpt-5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In LangGraph, the state is a Python dictionary (often explicitly typed using TypedDict) that holds the data as it moves through the workflow. Each node (function) in the StateGraph reads from and updates this state, passing it along to the next step. \n",
    "\n",
    "- This allows information like inputs, intermediate computations, and outputs to be shared and modified as your workflow progresses. \n",
    "\n",
    "- In short, the state is the single source of truth representing your workflow's current data at any point in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the structure of the state as it flows through the graph.\n",
    "class LLMState(TypedDict):\n",
    "    question: str   # The input question\n",
    "    answer: str     # The model's answer (to be filled in by our node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a node function: given a state with a 'question', it generates an 'answer' using the LLM.\n",
    "def llm_qa(state: LLMState) -> LLMState:\n",
    "    question = state['question']  # Extract the question\n",
    "    \n",
    "    # Format the input prompt for the language model\n",
    "    prompt = f\"Answer the following question {question}\"\n",
    "    \n",
    "    # Invoke the OpenAI model, extracting the answer from the response\n",
    "    answer = model.invoke(prompt).content\n",
    "\n",
    "    # Store the answer back in the state\n",
    "    state['answer'] = answer\n",
    "\n",
    "    return state  # Return the updated state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the workflow graph using our state structure.\n",
    "graph = StateGraph(LLMState)\n",
    "\n",
    "# Add our LLM node to the graph; this is the only computational step in this pipeline.\n",
    "graph.add_node('llm_qa', llm_qa)\n",
    "\n",
    "# Define the workflow edges (execution order).\n",
    "graph.add_edge(START, 'llm_qa')      # Start → llm_qa\n",
    "graph.add_edge('llm_qa', END)        # llm_qa → End\n",
    "\n",
    "# Compile the workflow so that it's ready to run.\n",
    "workflow = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the initial state with the question we want answered.\n",
    "initial_state = {'question': 'What is the Future of Agentic AI?'}\n",
    "\n",
    "# Invoke (execute) the graph pipeline with our initial state.\n",
    "final_state = workflow.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short answer: Agentic AI is moving from chatbots to reliable, goal‑directed software that can plan, use tools, coordinate with other agents, and act on your behalf under guardrails. In the near term it will automate many digital workflows with human oversight; over the medium term it will coordinate complex projects and increasingly control physical systems. The bottlenecks are reliability, safety, and governance—not raw capability.\n",
      "\n",
      "Key shifts to expect\n",
      "- From copilots to co‑workers: Agents will run multi‑step tasks end‑to‑end (research → decision support → execution → reporting), pausing for approval only at risk boundaries.\n",
      "- Stronger reasoning and planning: Architectures that separate planning from acting, search‑augmented decoding, verifiable tool use, and program‑of‑thought methods will reduce errors on long‑horizon tasks.\n",
      "- Tool use as a first‑class capability: Standardized tool interfaces (e.g., function calling, model context protocols) will let agents compose software, data, and services dynamically. Expect “latent tool use” where models decide which tools to call during generation.\n",
      "- Memory and personalization: Persistent, audited memory spanning episodic (what happened), semantic (facts), and preference layers, backed by vector and graph stores. Reflection and self‑correction loops will be routine.\n",
      "- Multi‑agent systems: Teams of specialized agents will collaborate, negotiate, and critique each other, improving robustness and throughput. Markets of “service agents” will emerge inside organizations.\n",
      "- Embodiment and real‑world action: Vision‑language‑action models will plan while lower‑level controllers execute, enabling warehouse, home, and field robotics. Simulation‑to‑real transfer and safety envelopes will be critical.\n",
      "- On‑device and private agents: Distilled/quantized models and efficient planners will run locally for latency, cost, and privacy, syncing securely with cloud tools as needed.\n",
      "- Enterprise integration: Agentic RPA will tie into CRMs, ERPs, IDP, and data warehouses. Agents will be KPI‑ and policy‑driven, with audit trails, approvals, and segregation of duties.\n",
      "- AgentOps becomes a stack: Planning graphs/state machines, tool catalogs, sandboxes, policy engines, observability, evaluation suites, deterministic replay, cost/latency controls, and incident response.\n",
      "- Governance and trust: Capability‑based permissions, least‑privilege tool access, network egress control, data provenance, policy compliance (e.g., AI Act‑style regimes), and third‑party audits will be standard.\n",
      "\n",
      "Near-term outlook (12–24 months)\n",
      "- Personal: Calendar/email/travel, research and shopping, web form filling, PC/app control, family logistics—with explicit permissions and transparent logs.\n",
      "- Workflows: Customer support triage/resolution, sales ops, finance reconciliations, BI report generation, QA/test authoring, code maintenance and small features. Human review at defined checkpoints.\n",
      "- Reliability: Well‑scoped tasks can hit “four nines” with guardrails, sandboxing, and post‑checks; open‑web tasks will remain brittle without containment and verification.\n",
      "- Security: Prompt injection and tool abuse mitigation by default: allowlists, structured IO, typed tool schemas, content firewalls, and environment sandboxes.\n",
      "\n",
      "Mid-term outlook (3–5 years)\n",
      "- Complex project agents: Multi‑week initiatives (e.g., data migrations, product launches) coordinated by agent teams with budget/time/resource management and stakeholder communication.\n",
      "- Cross‑organizational agent ecosystems: Contracted APIs between firms; agent‑to‑agent procurement and scheduling under enforceable policies.\n",
      "- Embodied services: Safe household and industrial tasks in constrained domains, with formal safety layers and certified competencies.\n",
      "- Regulation/maturity: Clearer liability, auditing, red‑team requirements, and incident reporting. Model/tool provenance and usage attestations become part of compliance.\n",
      "\n",
      "Hard problems that decide the pace\n",
      "- Guarantees and verification: Proving correctness of plans and outputs; coupling LLMs to typed DSLs and checkers; robust post‑execution validation.\n",
      "- Long‑horizon credit assignment: Learning from sparse feedback and outcomes over days/weeks; scalable oversight and synthesized training signals.\n",
      "- Distribution shift and fragility: Changing web UIs/APIs; adversarial content; non‑stationary data.\n",
      "- Safety and security: Preventing data exfiltration, unauthorized actions, model manipulation; keeping humans in control; aligning persistent objectives with user intent.\n",
      "- Cost/latency: Efficient inference, caching, and planning; when to think more vs act; on‑device/offload trade‑offs.\n",
      "\n",
      "How to prepare or build now\n",
      "- Start with narrow, high‑ROI workflows; design for approval gates and rollbacks.\n",
      "- Use explicit tool schemas, typed interfaces, and sandboxes; adopt least privilege and network egress controls.\n",
      "- Add memory deliberately: what to retain, for how long, with redaction and user controls.\n",
      "- Instrument everything: tracing, metrics, evaluations on real tasks, deterministic replays.\n",
      "- Treat prompts/plans as code: version, test, stage, and monitor; maintain a library of test scenarios and adversarial cases.\n",
      "\n",
      "Bottom line\n",
      "Agentic AI will increasingly function as a dependable digital workforce and, gradually, physical workforce assistant. The winners will pair strong models with disciplined engineering: tool-centric design, safety by construction, rigorous evaluation, and clear human governance.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print out the answer provided by the LLM.\n",
    "print(final_state['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraphvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
